{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivatives in Theano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following:\n",
    "\n",
    "http://deeplearning.net/software/theano/tutorial/gradients.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function which computes the derivative of some expression `y` w.r.t. `x`. We will use the macro `T.grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano import pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'((fill((x ** TensorConstant{2}), TensorConstant{1.0}) * TensorConstant{2}) * (x ** (TensorConstant{2} - TensorConstant{1})))'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = T.dscalar('x')\n",
    "y = x ** 2\n",
    "gy = T.grad(y, x)\n",
    "pp(gy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = theano.function([x], gy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(8.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(188.4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(94.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: we can't `pp(f)` - functions have no `.owner`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The optimizer simplifies the gradient expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(TensorConstant{2.0} * x)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp(f.maker.fgraph.outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "theano.compile.function_module.Function"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xx = np.linspace(0, 1, 10)\n",
    "yy = [f(x0) for x0 in xx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x108de1e90>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAEACAYAAACnJV25AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOKMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x108cf9a50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(xx, yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute the gradient of complex expressions such as the logistic function. It turns out that:\n",
    "\\begin{equation}\n",
    "\\frac{ds}{dx} = s(x) \\cdot (1 - s(x))\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = T.dmatrix('x')\n",
    "s = T.sum(1 / (1 + T.exp(-x)))\n",
    "gs = T.grad(s, x)\n",
    "dlogistic = theano.function([x], gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.25      ,  0.19661193],\n",
       "       [ 0.19661193,  0.10499359]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogistic([[0, 1], [-1, -2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, for any **scalar** expression `s`, `T.grad(s, w)` provides the Theano expression for computing $\\frac{\\partial s}{\\partial w}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The second argument of `T.grad` can be a list, in which case the output is also a list. For example:\n",
    "\n",
    "    gw, gb = T.grad(cost, [w, b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Jacobian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Theano's parlance, the _Jacobian_ is the tensor comprising the first partial derivatives of the output of a function with respect to its inputs. (This is a generalization of the Jacobian matrix.) The `theano.gradient.jacobian()` macro does everything needed to compute the Jacobian. But, we can also do it \"by hand\"..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To manually compute the Jacobian we need to use `scan` and loop over the entries in our function `y` and compute the gradient with respect to `x`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: `scan` is a generic op in Theano that allows writing in a symbolic manner all kinds of recurrent equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/perdue/anaconda/envs/python2/lib/python2.7/site-packages/theano/gof/cmodule.py:293: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility\n",
      "  rval = __import__(module_name, {}, {}, [module_name])\n"
     ]
    }
   ],
   "source": [
    "x = T.dvector('x')\n",
    "y = x ** 2\n",
    "J, updates = theano.scan(lambda i, y, x: T.grad(y[i], x), sequences=T.arange(y.shape[0]), non_sequences=[y, x])\n",
    "f = theano.function([x], J, updates=updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8.,  0.],\n",
       "       [ 0.,  8.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f([4, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code generates a sequence of _ints_ from 0 to `y.shape[0]` using `T.arange`. Then we loop through the sequence and at each step we compute the gradient of element `y[i]` with respect to `x`. `scan` concatenates all these rows, generating a maxtrix that corresponds to the Jacobian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([4, 4]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([4, 4]).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(np.array([4, 4]).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.array([4, 4])\n",
    "b = a ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here, `T.arange(y.shape[0])` ultimately is `[0, 1]`. The gradient is just `2x`, and the off-diagonals are zero because we are differentiating w.r.t. `x_1` and then `x_2`, but there are no cross terms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8.,  0.,  0.,  0.],\n",
       "       [ 0.,  8.,  0.,  0.],\n",
       "       [ 0.,  0.,  8.,  0.],\n",
       "       [ 0.,  0.,  0.,  8.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f([4, 4, 4, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "for{cpu,scan_fn}.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theano.gradient.jacobian(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jj = theano.gradient.jacobian(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fj = theano.function([x], jj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8.,  0.],\n",
       "       [ 0.,  8.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fj([4, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = T.dvector('x')\n",
    "y = T.dvector('y')\n",
    "z = x ** 2 + y ** 2 + 2 * x * y\n",
    "jz = theano.gradient.jacobian(z, [x, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fjz = theano.function([x, y], jz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 10.,   0.,   0.],\n",
       "        [  0.,  14.,   0.],\n",
       "        [  0.,   0.,  18.]]), array([[ 10.,   0.,   0.],\n",
       "        [  0.,  14.,   0.],\n",
       "        [  0.,   0.,  18.]])]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fjz([1, 2, 3], [4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[  8.,   0.],\n",
       "        [  0.,  12.]]), array([[  8.,   0.],\n",
       "        [  0.,  12.]])]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fjz([1, 2], [3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 8.,  0.],\n",
       "        [ 0.,  8.]]), array([[ 8.,  0.],\n",
       "        [ 0.,  8.]])]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fjz([2, 2], [2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = T.dmatrix('x')\n",
    "y = T.dmatrix('y')\n",
    "z = T.flatten(x ** 2 * y ** 2)\n",
    "jz = theano.gradient.jacobian(z, [x, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 8.,  0.],\n",
       "        [ 0.,  8.]]), array([[ 8.,  0.],\n",
       "        [ 0.,  8.]])]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fjz([2, 2], [2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Hessian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Theano, the _Hessian_ has the usual mathematical meaning: it is the matrix comprising the second order partial derivative of a function with scalar output and vector input. `theano.gradient.hessian()` does all that is needed to compute the Hessian, but we may also do it manually.\n",
    "\n",
    "The manual computation of the Hessian is similar to that of the Jacobian. The difference now is that, instead of computing the Jacobian of some expression `y`, we compute the Jacobian of `T.grad(cost, x)` where `cost` is a scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = T.dvector('x')\n",
    "y = x ** 2\n",
    "cost = y.sum()\n",
    "gy = T.grad(cost, x)\n",
    "H, updates = theano.scan(lambda i, gy, x: T.grad(gy[i], x), sequences=T.arange(gy.shape[0]), non_sequences=[gy, x])\n",
    "f = theano.function([x], H, updates=updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.,  0.],\n",
       "       [ 0.,  2.]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f([4, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Recall that in maximum likelihood estimation, solving the equation where we set the gradient of the likelihood function equal to zero (the so-called \"estimating equation\") provides estimates of the coefficients of the model. The negative of the inverse of the Hessian matrix produces the estimated variance-covariance matrix of the parameter estimates of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacobian times a vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we can express algorithms in terms of Jacobians times vectors or vectors times Jacobians. In these cases, it is sometimes faster to evaluate the composite than it is to find the Jacobian first and then multiply. We would like Theano to identify these patterns for us, but this is a difficult general problem. Therefore, there are special functions for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R-operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _R operator_ is built to evaluate the product between a Jacobian and a vector, $ \\frac{\\partial f(x)}{\\partial x} v$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = T.dmatrix('W')\n",
    "V = T.dmatrix('V')\n",
    "x = T.dvector('x')\n",
    "y = T.dot(x, W)\n",
    "JV = T.Rop(y, W, V)\n",
    "f = theano.function([W, V, x], JV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  2.])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_1 = [[1, 1], [1, 1]]\n",
    "V_1 = [[2, 2], [2, 2]]\n",
    "x_1 = [0, 1]\n",
    "f(W_1, V_1, x_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L-operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _L operator_ computes a _row_ vector times the Jacobian, $v \\frac{\\partial f(x)}{\\partial x}$. The _L operator_ is also supported for generic tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = T.dmatrix('W')\n",
    "v = T.dvector('v')\n",
    "x = T.dvector('x')\n",
    "y = T.dot(x, W)\n",
    "VJ = T.Lop(y, W, v)\n",
    "f = theano.function([v, x], VJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.],\n",
       "       [ 2.,  2.]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f([2, 2], [0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why don't we have to say what `W` is in the example above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hessian times a vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we need to compute the Hessian times a vector, we can make use of the above operators to do so more efficiently. Due to the symmetry of the Hessian, we have two methods that give the same result and we should profile to choose the optimal method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = T.dvector('x')\n",
    "v = T.dvector('v')\n",
    "y = T.sum(x ** 2)\n",
    "gy = T.grad(y, x)\n",
    "vH = T.grad(T.sum(gy * v), x)\n",
    "f = theano.function([x, v], vH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.,  4.])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f([4, 4], [2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, using the _R operator_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = T.dvector('x')\n",
    "v = T.dvector('v')\n",
    "y = T.sum(x ** 2)\n",
    "gy = T.grad(y, x)\n",
    "Hv = T.Rop(gy, x, v)\n",
    "f = theano.function([x, v], Hv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.,  4.])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f([4, 4], [2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
