{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theano Tutorial - Python Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following http://deeplearning.net/software/theano/tutorial/python-memory-management.html#python-memory-management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Memory Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python allocates memory transparently, manages objects using a reference counting system, and frees memory when an object's reference count falls to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the size of an `int`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def show_sizeof(x, level=0):\n",
    "    print \"\\t\" * level, x.__class__, sys.getsizeof(x), x\n",
    "    \n",
    "    if hasattr(x, '__iter__'):\n",
    "        if hasattr(x, 'items'):\n",
    "            for xx in x.items():\n",
    "                show_sizeof(xx, level + 1)\n",
    "        else:\n",
    "            for xx in x:\n",
    "                show_sizeof(xx, level + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <type 'NoneType'> 16 None\n",
      " <type 'int'> 24 3\n",
      " <type 'long'> 36 9223372036854775808\n",
      " <type 'long'> 40 1857574678926579826398562938\n",
      " <type 'long'> 56 185757467892657982639856293887653985698276439856928346598236598623984569\n"
     ]
    }
   ],
   "source": [
    "show_sizeof(None)\n",
    "show_sizeof(3)\n",
    "show_sizeof(2**63)\n",
    "show_sizeof(1857574678926579826398562938)\n",
    "show_sizeof(185757467892657982639856293887653985698276439856928346598236598623984569)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <type 'float'> 24 3.14159265359\n"
     ]
    }
   ],
   "source": [
    "show_sizeof(3.14159265358979323846264338327950288)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `float` is three times the size a C programmer would expect!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <type 'str'> 37 \n",
      " <type 'str'> 66 My hovercraft is full of eels\n"
     ]
    }
   ],
   "source": [
    "show_sizeof(\"\")\n",
    "show_sizeof(\"My hovercraft is full of eels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <type 'list'> 72 []\n",
      " <type 'list'> 96 [4, 'toaster', 230.1]\n",
      "\t<type 'int'> 24 4\n",
      "\t<type 'str'> 44 toaster\n",
      "\t<type 'float'> 24 230.1\n"
     ]
    }
   ],
   "source": [
    "show_sizeof([])\n",
    "show_sizeof([4, \"toaster\", 230.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of an empty C++ `std::list()` is only 16 bytes, 4-5 times less than the Python \"equivalent.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <type 'dict'> 280 {}\n",
      " <type 'dict'> 280 {'a': 213, 'b': 2131}\n",
      "\t<type 'tuple'> 72 ('a', 213)\n",
      "\t\t<type 'str'> 38 a\n",
      "\t\t<type 'int'> 24 213\n",
      "\t<type 'tuple'> 72 ('b', 2131)\n",
      "\t\t<type 'str'> 38 b\n",
      "\t\t<type 'int'> 24 2131\n"
     ]
    }
   ],
   "source": [
    "show_sizeof({})\n",
    "show_sizeof({'a': 213, 'b': 2131})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary doesn't quite \"add up\" because there is internal storage for a tree-like structure or a hash table. The C++ `std::map()` takes 48 bytes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this mean? If we need to scale, we need to be careful about how many objects we create to limit the quantity of memory our program uses. However, to devise a good memory management strategy, we need to consider not only the sizes of the objects, but how many and the order in which we create them. A key element to understand is how Python allocates memory internally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Internal Memory Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed up memory allocation (and reuse), Python uses a number of lists for small objects. Each list contains objects of similar size: a list for objects 1-8 bytes in size, a list fo 9-16, and so on. When a small object needs to be created, we either re-use a free block in the list or allocate a new one.\n",
    "\n",
    "There are details to that management, but they aren't important. If interested, see\n",
    "\n",
    "http://www.evanjones.ca/memoryallocator/\n",
    "\n",
    "The important point is that those lists _never shrink_.\n",
    "\n",
    "If an item (of size `x`) is deallocated (freed by lack of reference), its location is not returned to Python's global memory pool, but merely marked as free and added to the free list of items of size `x`. The dead object's location will be re-used if another object of similar size is needed, and if there are no dead objects, new space is allocated.\n",
    "\n",
    "Therefore, the memory footprint of the application is dominated by the largest number of small objects allocated at any given point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we should allocated only the number of small objects necessary for one task, favoring (otherwise _non-Pythonic_) loops where only a small number of elements are created or processed rather than the (more Pythonic) patterns where lists are created using list generation syntax and then processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The free list only growing may not seem like a problem because the memory is still accessible to the Python program. But, because Python only returns memory to the OS on the heap on Windows, on Linux we will only ever see the total memory used by the program increase.\n",
    "\n",
    "See https://github.com/fabianp/memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: memory-profile-me.py\n",
      "\n",
      "Line #    Mem usage    Increment   Line Contents\n",
      "================================================\n",
      "     5   62.418 MiB    0.000 MiB   @profile\n",
      "     6                             def function():\n",
      "     7  101.098 MiB   38.680 MiB       x = list(range(1000000))\n",
      "     8  188.633 MiB   87.535 MiB       y = copy.deepcopy(x)\n",
      "     9  188.633 MiB    0.000 MiB       del x\n",
      "    10  188.633 MiB    0.000 MiB       return y\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run -m memory_profiler memory-profile-me.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, at the command line:\n",
    "\n",
    "    (python2)TheanoScratch$ python -m memory_profiler memory-profile-me.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory can increase suprisingly quickly if you are not careful!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is `pickle` wasteful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: test-pickle.py\n",
      "\n",
      "Line #    Mem usage    Increment   Line Contents\n",
      "================================================\n",
      "    10  188.781 MiB    0.000 MiB   @profile\n",
      "    11                             def create_file():\n",
      "    12  188.781 MiB    0.000 MiB       x = [(random.random(),\n",
      "    13                                       random_string(),\n",
      "    14                                       random.randint(0, 2 ** 64))\n",
      "    15  331.352 MiB  142.570 MiB            for _ in xrange(1000000)]\n",
      "    16                             \n",
      "    17  578.938 MiB  247.586 MiB       pickle.dump(x, open('machin.pkl', 'w'))\n",
      "\n",
      "\n",
      "Filename: test-pickle.py\n",
      "\n",
      "Line #    Mem usage    Increment   Line Contents\n",
      "================================================\n",
      "    20  317.871 MiB    0.000 MiB   @profile\n",
      "    21                             def load_file():\n",
      "    22  621.039 MiB  303.168 MiB       y = pickle.load(open('machin.pkl', 'r'))\n",
      "    23  621.039 MiB    0.000 MiB       return y\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run -m memory_profiler test-pickle.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somehow, _pickling_ is very bad for memory consumption. Unpickling, on the other hand, seems fairly efficient. Overall, pickling should be avoided for memory-sensitive applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "If we profile `test-flat.py`, we can see we use a lot less memory (but, we won't here because it takes a long time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import memory_profiler\r\n",
      "import random\r\n",
      "\r\n",
      "\r\n",
      "def random_string():\r\n",
      "    return \"\".join([chr(64 + random.randint(0, 25)) for _ in xrange(20)])\r\n",
      "\r\n",
      "\r\n",
      "@profile\r\n",
      "def create_file():\r\n",
      "    x = [(random.random(),\r\n",
      "          random_string(),\r\n",
      "          random.randint(0, 2 ** 64))\r\n",
      "         for _ in xrange(1000000)]\r\n",
      "\r\n",
      "    f = open('machin.flat', 'w')\r\n",
      "    for xx in x:\r\n",
      "        print >>f, xx\r\n",
      "    f.close()\r\n",
      "\r\n",
      "\r\n",
      "@profile\r\n",
      "def load_file():\r\n",
      "    y = []\r\n",
      "    f = open('machin.flat', 'r')\r\n",
      "    for line in f:\r\n",
      "        y.append(eval(line))\r\n",
      "    f.close()\r\n",
      "    return y\r\n",
      "\r\n",
      "\r\n",
      "if __name__ == '__main__':\r\n",
      "    create_file()\r\n",
      "    load_file()\r\n"
     ]
    }
   ],
   "source": [
    "cat test-flat.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This all generalizes to strategies where we don't load the whole dataset at once, but rather load a bit at a time. Loading data to a NumPy array for example, should invlve creating the array and then reading the file line by line to fill the array - this allocates one copy of the whole data. If we use `pickle`, we allocate the whole data set (at least) twice: once by `pickle` and once by NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the tutorial on loading and saving:\n",
    "\n",
    "http://deeplearning.net/software/theano/tutorial/loading_and_saving.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python's design goals are very different than C's. C is designed for granular control, Python for programmer speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
