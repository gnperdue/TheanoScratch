{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following:\n",
    "\n",
    "http://deeplearning.net/tutorial/mlp.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An MLP may be viewed as a logistic regression classifier where the input is first transformed using a learned non-linear transformation $\\Phi$. The transformation projects the input data into a space where it becomes linearly separable. This intermediate layer is referred to as a _hidden layer_. A single hidden layer is sufficient to make MLPs _universal approximators_. There are substantial benefits to using many hidden layers, however - this is the premise of **deep learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally, a one-hidden-layer MLP is a function $f: R^D \\to R^L$ where $D$ is the size of the input vector $x$ and $L$ is the size of the output vector such that\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = G(b^{(2)} + W^{(2)} (s (b^{(1)} + W^{(1)} x)))\n",
    "\\end{equation}\n",
    "\n",
    "with bias vectors $b^{(1)}$ and $b^{(2)}$, weight matrices $W^{(1)}$ and $W^{(2)}$, and activation functions $G$ and $s$. The vector $h(x) = \\Phi(x) = s(b^{(1)} + W^{(1)}x)$ constitutes the hidden layer. $W^{(1)} \\in R^{D \\times D_h}$ is the weight matrix connecting the input vector to the hidden layer. Each _column_ $W_{i}^{(1)}$ represents the weights from the input units to the $i$th hidden unit. Typical choices for $s$ include $tanh(a) = (e^a - e^{-a}) / (e^a + e^{-a})$ or the logistic sigmoid function $\\sigma(a) = 1 / (1 + e^{-a})$. Both $tanh$ and $sigmoid$ are scalar to scalar functions, and their natural extension to vectors and tensors consists in applying them element-wise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output vector is obtained as\n",
    "\n",
    "\\begin{equation}\n",
    "o(x) = G(b^{(2)} + W^{(2)} h(x))\n",
    "\\end{equation}\n",
    "\n",
    "This is the same form as used in regular logistic regression classification. We choose class-membership probabilities based on the _softmax_ function (for multi-class classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train an MLP we learn _all_ the parameters of the model using stochastic gradient descent with minibatches. We obtain gradients of the parameters using the _backpropagation algorithm_, which is a special case of the chain-rule. Theano performs these calculations symbolically automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Going from logistic regression to MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by implementing a class to represent a hidden layer. The initial values for the weights of a hidden layer $i$ should be uniformly sampled from a symmetric interval that depends on the activation function. For $tanh$, according to [Xavier10](http://deeplearning.net/tutorial/references.html#xavier10), the interval should be:\n",
    "\n",
    "\\begin{equation}\n",
    "\\left[ -\\sqrt{\\frac{6}{fan_{in} + fan_{out}}}, \\sqrt{\\frac{6}{fan_{in} + fan_{out}}} \\right]\n",
    "\\end{equation}\n",
    "\n",
    "where $fan_{in}$ is the number of units in the $(i-1)$th layer and $fan_{out}$ is the number of units in the $i$th layer. For the sigmoid, the interval is\n",
    "\n",
    "\\begin{equation}\n",
    "\\left[ -4\\sqrt{\\frac{6}{fan_{in} + fan_{out}}}, 4\\sqrt{\\frac{6}{fan_{in} + fan_{out}}} \\right]\n",
    "\\end{equation}\n",
    "\n",
    "This initialization ensures that, early in the training, each neuron operates in a regime of its own activation function where information can easily be propagated both upward (activations flowing from inputs to outputs) and backward (gradients flowing from outputs to inputs).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also use [L1 and L2 regularization](http://deeplearning.net/tutorial/gettingstarted.html#l1-l2-regularization). For this, we need to compute the L1 norm and the squared L2 norm of the weights $W^{(1)}$ and $W^{(2)}$.\n",
    "\n",
    "As before, we train this model using stochastic gradient descent with mini-batches. The difference now is that we modify the cost function to include the regularization term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
