{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoising Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following: http://deeplearning.net/tutorial/dA.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also see: http://www.iro.umontreal.ca/~lisa/publications2/index.php/publications/show/217 for more on dA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See Section 4.6 in http://www.iro.umontreal.ca/~lisa/publications2/index.php/publications/show/239 for a discussion of autoencoders. Essentially, an autoencoder takes an input $x \\in [0, 1]^d$ and first maps it (with an _encoder_) to a hidden representation, $y \\in [0, 1]^{d'}$ through a deterministic mapping, e.g.\n",
    "\n",
    "\\begin{equation}\n",
    "y = s(W x + b)\n",
    "\\end{equation}\n",
    "\n",
    "where $s$ is a non-linearity (e.g. sigmoid). The latent representation, or code, is then mapped back (with a _decoder_) into a reconstruction $z$ that is the same shape as $x$:\n",
    "\n",
    "\\begin{equation}\n",
    "z = s(W' y + b')\n",
    "\\end{equation}\n",
    "\n",
    "where $W'$ is not the transpose of $W$, but rather a different matrix. $z$ is a _prediction_ of $x$, given the code $y$. Optionally, we _may_ constrain $W' = W^T$. The parameters of the model are optimized to minimize the average reconstruction error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reconstruction error may be measured in a few differnt ways. The traditional _squared error_ $L(x, z) = |x - z|^2$ may be used. If the input is interpreted as either bit vectors or vectors of bit probabilities, the _cross-entropy_ may be used:\n",
    "\n",
    "\\begin{equation}\n",
    "L_H(x, z) = - \\sum_{k=1}^d \\left[ x_k \\log z_k + (1 - x_k) \\log (1 - z_k) \\right]\n",
    "\\end{equation}\n",
    "\n",
    "The hope is that the code $y$ is a distributed representation that captures the coordinates along the main factors of variation in the data. This is similar to the way the projection on principal components captures the main factors of variation in the data. Indeed, in the case of one hidden layer and the mean squared error is used to train the network, the $k$ hidden units learn to prject the input in the span of the first $k$ principal components of the data. If the hidden layer is non-linear, it behaves differently from PCA and may capture multi-modal aspects of the input distribution. Stacking multiple encoders and decoders - building a deep auto-encoder - leads to even further divergance from PCA. See: http://www.cs.toronto.edu/~rsalakhu/papers/science.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Because $y$ acts like a lossy compression of $x$, it cannot be a small-loss compression for all $x$. Optimization makes it perform well for training examples, but not arbitrary inputs. Auto-encoders generalize by giving low reconstruction error on test samples from the same distribution as the training examples, although they give generally high error on samples chosen randomly from the input space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement an auto-encoder in such a fashion as to make it stackable. Because we are using tied weights here, we will use $W^T$ for $W'$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
