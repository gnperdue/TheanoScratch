{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying MNIST digits using Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following:\n",
    "\n",
    "http://deeplearning.net/tutorial/logreg.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a linear probabilistic classifier. Classification is performed by projecting an input vector onto a set of hyperplanes, and the distance from the input to a hyperplane quantifies the probability the input vector is a member of the corresponding class.\n",
    "\n",
    "The probability an input vector $x$ is a member of class $i$ may be written as:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "P(Y = i|x,W,b) =&~ \\text{softmax}_i (W x + b) \\\\\n",
    "=&~ \\frac{e^{W_i x + b_i}}{\\sum_j e^{W_j x + b_j}}\\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "where $W$ is a matrix of weights and $b$ is a bias vector.\n",
    "\n",
    "The model prediction is then\n",
    "\\begin{equation}\n",
    "y_p = \\text{argmax}_i P(Y = i|x, W, b)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Theano, because the parameters of our model must maintain a persistent state throughout training, we will allocate shared variables for $W$ and $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning optimal parameters is the process of minimizing a loss function. For multi-class logistic regression, the negative log-likelihood is a common choice for the loss function. It is equivalent to maximizing the likelihood of the data set under the model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\mathcal{L}(\\theta = \\left\\{W, b\\right\\}, \\mathcal{D}) =&~ \\sum_{i=0}^{\\mathcal{D}} \\log P(Y = y^{(i)} | x^{(i)}, W, b) \\\\\n",
    "\\mathcal{l}(\\theta = \\left\\{W, b\\right\\}, \\mathcal{D}) =&~ -\\mathcal{L}(\\theta = \\left\\{W, b\\right\\}, \\mathcal{D}) \\\\\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is the simplest method for minimizing arbitrary non-linear functions. For more, see:\n",
    "\n",
    "http://deeplearning.net/tutorial/gettingstarted.html#opt-sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Even though the loss is formally defined as the _sum_ over the dataset of individual error terms, in the code we will usually use `T.mean`. This allows for less dependence on minibatch size in the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a LogisticRegression class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by allocating symbolic variables for the training imputs $x$ and their classes $y$ (note `x` and `y` are defined outside the scope of the object). We also define a symbolic `cost` variable to minimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Theano we do not need to manually derive expressions for the gradient of the loss function with respect to the parameters because Theano performs automatic differentiaion. We can use these gradients in training by defining the `updates` of our function to continually shift the parameters by the `learning_rate` times their gradients.\n",
    "\n",
    "We also use `givens` to pass data into our function more efficiently. \n",
    "\n",
    "Our function `train_model` is defined such that:\n",
    "\n",
    "* the input is the minibatch `index` that, with the batch size, defines `x` and `y`\n",
    "* the return value is the cost/loss associated with the `x` and `y` defined by the `index`\n",
    "* on each call, our function first replaces `x` and `y` with `index`ed slices from the data, and then it evaluates the cost of that minibatch and applies the operations we defined in the `updates` list\n",
    "\n",
    "So each time we call `train_model(index)`, we compute and return the cost of a minibatch while performing a step of MSGD (Minibatch SGD). The algorithm therefore is looping over all the examples in the dataset, considering them in one minibatch at a time, and repeatedly calling `train_model`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When testing the model we are interested in the likelihood but also the plain number of misclassified examples. So our `LogisticRegression` class will therefore have an extra instance method that builds the symbolic graph for retrieving the number of misclassified examples in each minibatch.\n",
    "\n",
    "We then create functions `test_model` and `validate_model` that we can use to retrieve this value. These functions take a minibatch and compte the number of items that were misclassified. The only difference between the functions is that `test_model` draws its data from the testing set and `validate_model` draws its data from the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
